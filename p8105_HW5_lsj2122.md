P8105 Homework 5
================
Laylah Jones
2023-11-15

# Problem 1

## Load and clean the dataset

``` r
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))
```

``` r
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Philadelphia, PA")
```

The raw dataset has 49142 homicides between 2010 and 2016 and 14
variables that include uid, reported_date, victim_last, victim_first,
victim_race, victim_age, victim_sex, city, state, lat, lon, disposition,
city_state, resolution

``` r
city_homicide_df = 
  homicide_df |> 
  select(city_state, disposition, resolution) |>  
  group_by(city_state) |> 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

``` r
atl_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Atlanta, GA") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Atlanta, GA") %>% pull(hom_total)) 

broom::tidy(atl_test) %>% 
  knitr::kable(digits = 3)
```

| estimate | statistic | p.value | parameter | conf.low | conf.high | method                                               | alternative |
|---------:|----------:|--------:|----------:|---------:|----------:|:-----------------------------------------------------|:------------|
|    0.383 |    52.493 |       0 |         1 |    0.353 |     0.415 | 1-sample proportions test with continuity correction | two.sided   |

I am focusing only on Atlanta, GA and use the `prop.test` and
`broom::tidy` function to get an estimate and CI of the number of
unsolved homicides.

## Running Prop.Test

``` r
results_df = 
  city_homicide_df |>  
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>  
  select(-prop_tests) |>  
  unnest(tidy_tests) |>  
  select(city_state, estimate, conf.low, conf.high) |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_tests = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

## Plot of Estimates and CIs for Each City

``` r
results_df |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |>  
  ggplot(aes(
    x = city_state, 
    y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(
    ymin = conf.low, 
    ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

<img src="p8105_HW5_lsj2122_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

This plot shows the rate in which homicides are solved by city. We can
note that Chicago, IL has the highest, from which we can infer that it
is the location of majority of the homicides.

# Problem 2

## Load the data

``` r
study_df = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)) |> 
  mutate(data = map(path, read_csv)) |> 
  unnest(c(data))

study_df
```

    ## # A tibble: 52,199 × 22
    ##    files     path  week_1 week_2 week_3 week_4 week_5 week_6 week_7 week_8 uid  
    ##    <chr>     <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>
    ##  1 con_01.c… data…   0.2   -1.31   0.66   1.96   0.23   1.09   0.05   1.94 <NA> 
    ##  2 con_02.c… data…   1.13  -0.88   1.07   0.17  -0.83  -0.31   1.58   0.44 <NA> 
    ##  3 con_03.c… data…   1.77   3.11   2.22   3.26   3.31   0.89   1.88   1.01 <NA> 
    ##  4 con_04.c… data…   1.04   3.66   1.22   2.33   1.47   2.7    1.87   1.66 <NA> 
    ##  5 con_05.c… data…   0.47  -0.58  -0.09  -1.37  -0.32  -2.17   0.45   0.48 <NA> 
    ##  6 con_06.c… data…   2.37   2.5    1.59  -0.16   2.08   3.07   0.78   2.35 <NA> 
    ##  7 con_07.c… data…   0.03   1.21   1.13   0.64   0.49  -0.12  -0.07   0.46 <NA> 
    ##  8 con_08.c… data…  -0.08   1.42   0.09   0.36   1.18  -1.16   0.33  -0.44 <NA> 
    ##  9 con_09.c… data…   0.08   1.24   1.44   0.41   0.95   2.75   0.3    0.03 <NA> 
    ## 10 con_10.c… data…   2.14   1.15   2.52   3.44   4.26   0.97   2.73  -0.53 <NA> 
    ## # ℹ 52,189 more rows
    ## # ℹ 11 more variables: reported_date <dbl>, victim_last <chr>,
    ## #   victim_first <chr>, victim_race <chr>, victim_age <chr>, victim_sex <chr>,
    ## #   city <chr>, state <chr>, lat <dbl>, lon <dbl>, disposition <chr>

## Tidy the data

``` r
tidy_study_df = 
  study_df |> 
  separate(files, into = c("control_arm", "subject_ID", "csv")) |> 
  mutate(
    control_arm = ifelse(control_arm == "con", "control", control_arm),
    control_arm = ifelse(control_arm == "exp", "experimental", control_arm)
  ) |> 
  pivot_longer(
    cols = starts_with("week"), 
    names_to = "week", 
    names_prefix = "week_",
    values_to = "observation") |> 
  select(subject_ID, control_arm, week, observation) |> 
  mutate(week = as.numeric(week))

tidy_study_df
```

    ## # A tibble: 417,592 × 4
    ##    subject_ID control_arm  week observation
    ##    <chr>      <chr>       <dbl>       <dbl>
    ##  1 01         control         1        0.2 
    ##  2 01         control         2       -1.31
    ##  3 01         control         3        0.66
    ##  4 01         control         4        1.96
    ##  5 01         control         5        0.23
    ##  6 01         control         6        1.09
    ##  7 01         control         7        0.05
    ##  8 01         control         8        1.94
    ##  9 02         control         1        1.13
    ## 10 02         control         2       -0.88
    ## # ℹ 417,582 more rows

## Spaghetti Plot of Observations on Each Subject Over Time

``` r
study_plot = 
  tidy_study_df |> 
  ggplot(aes(x = week, y = observation, color = subject_ID)) +
  geom_point() +
  geom_line() +
  facet_grid(.~control_arm) + 
  labs(
    x = "Week",
    y = "Observation",
    title = "Observations for Each Subject by Week"
  )

study_plot
```

    ## Warning: Removed 417432 rows containing missing values (`geom_point()`).

    ## Warning: Removed 417432 rows containing missing values (`geom_line()`).

<img src="p8105_HW5_lsj2122_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

From this plot, we can observe that the control arm had lower values
than the experimental arm. The control arm has observations that are
consistently between the range of approximately -2.17 and 4.26 for each
subject in all eight weeks, with the lowest observation value of -2.17
in week six for subject 5 and the highest observation value of 4.26 in
week five for subject 10. The experimental arm has observations that are
consistently between the range of approximately -0.84 and 7.66 for each
subject in all eight weeks, with the lowest observation value being in
week one for subject 2 and the highest observation value being in week
seven for subject 6.

# Problem 3

## Setting the Design Elements and Generating 5000 Datasets

``` r
n = 30
sigma = 5
mu = 0
mus = c(0, 1, 2, 3, 4, 5, 6)
alpha = 0.05
set.seed(12345)
datasets_number = 50


sim_mean_sd = function(n, mu, sigma) {
  
  sim_df = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_df |> 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}

t_test = function(n, mu, sigma) {
  sample = rnorm(n, mean = mu, sd = sigma)
  test_result = t.test(sample, mu = 0)
  broom::tidy(test_result)
}

results = 
  tibble(mu = numeric(), mu_hat = numeric(), p_value = numeric(), reject = logical())
```

``` r
for (mu in mus) {
  for (i in 1:50) {
    sim_results = sim_mean_sd(n, mu, sigma)
    t_test_results = t_test(n, mu, sigma)
    results = results |>  
      add_row(mu = mu, 
              mu_hat = sim_results$mu_hat, 
              p_value = t_test_results$p.value, 
              reject = t_test_results$p.value < alpha)
  }
}

power_results = results |>  
  group_by(mu) |>  
  summarize(power = mean(reject), 
            avg_mu_hat = mean(mu_hat), 
            avg_mu_hat_rejected = mean(mu_hat[reject]))
```

## Plot Showing the Proportion of Times the Null was Rejected

``` r
power_results |>  
  ggplot(aes(
    x = mu, 
    y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Proportion of Times the Null Hypothesis was Rejected for Each Mu", 
    x = "True Value of Mu", 
    y = "Power")
```

<img src="p8105_HW5_lsj2122_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

The plot shows a positive association between effect size and power.
From the plot, we can see that as the value of mu increases the
proportion of times the null hypothesis was rejected also increased.
When mu reaches a value of 5 and 6, the power is 1. This could be
interpreted as a larger effect size leading to higher power, meaning a
higher rejection rate.

## Plot Comparing the Average Estimate and the True Value of Mu

``` r
power_results |>  
  ggplot(aes(x = mu)) +
  geom_point(aes(y = avg_mu_hat, color = "Average Estimate"), shape = 1) +  
  geom_line(aes(y = avg_mu_hat, color = "Average Estimate")) +
  geom_point(aes(y = avg_mu_hat_rejected, color = "Conditional Average Estimate"), shape = 2) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Conditional Average Estimate"), linetype = "dashed") +
  labs(
    title = "Average Estimated Mean vs. True Mean",
    x = "True Mean (mu)",
    y = "Average Estimated Mean",
    color = "mu_hat"
  ) +
  scale_color_manual(
    values = c("Average Estimate" = "red", "Conditional Average Estimate" = "blue")
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

<img src="p8105_HW5_lsj2122_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

The plot shows that the sample average of the mean (mu) in the tests
where the null hypothesis was rejected is closely approximated to the
true mean (mu). This is demonstrated with the overlay of lines for
average estimate and conditional average estimate. This implies that
when the null hypothesis is rejected, the t-test serves as an unbiased
estimator of the population mean, aligning with the expected
characteristics of the t-test within the specified simulation
conditions.

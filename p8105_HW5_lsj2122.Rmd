---
title: "P8105 Homework 5"
author: "Laylah Jones"
date: 2023-11-15
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(rvest)
library(readr)
library(broom)
library(dplyr)

options(readr.show_col_types = FALSE)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = ("viridis"))

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Load and clean the dataset

```{r}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv", na = c("", "NA", "Unknown"))
```

```{r}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  ) %>% 
  filter(city_state != "Philadelphia, PA")
```

The raw dataset has `r nrow(homicide_df)` homicides between 2010 and 2016 and `r ncol(homicide_df)` variables that include `r colnames(homicide_df)`

```{r}
city_homicide_df = 
  homicide_df |> 
  select(city_state, disposition, resolution) |>  
  group_by(city_state) |> 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

```{r}
atl_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Atlanta, GA") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Atlanta, GA") %>% pull(hom_total)) 

broom::tidy(atl_test) %>% 
  knitr::kable(digits = 3)
```

I am focusing only on Atlanta, GA and use the `prop.test` and `broom::tidy` function to get an estimate and CI of the number of unsolved homicides. 

## Running Prop.Test

```{r}
results_df = 
  city_homicide_df |>  
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>  
  select(-prop_tests) |>  
  unnest(tidy_tests) |>  
  select(city_state, estimate, conf.low, conf.high) |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

## Plot of Estimates and CIs for Each City
```{r}
results_df |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |>  
  ggplot(aes(
    x = city_state, 
    y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(
    ymin = conf.low, 
    ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This plot shows the rate in which homicides are solved by city. We can note that Chicago, IL has the highest, from which we can infer that it is the location of majority of the homicides.

# Problem 2

## Load the data

```{r, col_types = FALSE}
study_df = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)) |> 
  mutate(data = map(path, read_csv)) |> 
  unnest(c(data))

study_df
```

## Tidy the data
```{r}
tidy_study_df = 
  study_df |> 
  separate(files, into = c("control_arm", "subject_ID", "csv")) |> 
  mutate(
    control_arm = ifelse(control_arm == "con", "control", control_arm),
    control_arm = ifelse(control_arm == "exp", "experimental", control_arm)
  ) |> 
  pivot_longer(
    cols = starts_with("week"), 
    names_to = "week", 
    names_prefix = "week_",
    values_to = "observation") |> 
  select(subject_ID, control_arm, week, observation) |> 
  mutate(week = as.numeric(week))

tidy_study_df
```


## Spaghetti Plot of Observations on Each Subject Over Time

```{r}
study_plot = 
  tidy_study_df |> 
  ggplot(aes(x = week, y = observation, color = subject_ID)) +
  geom_point() +
  geom_line() +
  facet_grid(.~control_arm) + 
  labs(
    x = "Week",
    y = "Observation",
    title = "Observations for Each Subject by Week"
  )

study_plot
```

The following spaghetti plot shows observations on each subject over 8 weeks. From this plot, we can observe that the control arm had lower values than the experimental arm. The control arm has observations that are consistently between the range of approximately -2.17 and 4.26 for each subject in all eight weeks, with the lowest observation value of -2.17 in week six for subject 5 and the highest observation value of 4.26 in week five for subject 10. The experimental arm has observations that are consistently between the range of approximately -0.84 and 7.66 for each subject in all eight weeks, with the lowest observation value being in week one for subject 2 and the highest observation value being in week seven for subject 6.


# Problem 3

## Setting the Design Elements and Generating 5000 Datasets

```{r, design_elements}
n = 30
sigma = 5
mu = 0
mus = c(0, 1, 2, 3, 4, 5, 6)
alpha = 0.05
set.seed(12345)
datasets_number = 5000
```

```{r, function}
sim_mean_sd = function(n, mu, sigma) {
  
   rnorm(n, mu, sigma) |> 
      t.test() |> 
      broom::tidy() |> 
      select(estimate, p.value)
}

sim_results =
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, sim_mean_sd, n = 30, sigma = 5)) |> 
  unnest(test_result)

results = 
  tibble(mu = numeric(), mu_hat = numeric(), p_value = numeric(), reject = logical())

sim_results |>
  head() |> 
  knitr::kable(digits = 3)
```

```{r, change_mu}
sim_results_2 = 
  expand_grid(
    mu = c(1:6),
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, sim_mean_sd, n = 30, sigma = 5)) |> 
  unnest(test_result)
```

## Plot Showing the Proportion of Times the Null was Rejected

```{r, plot_1}
sim_results_2 |>
  group_by(mu) |> 
  summarise(power = sum(p.value < 0.05) / n()) |>
  ggplot(aes(
    x = mu, 
    y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Proportion of Times the Null Hypothesis was Rejected for Each Mu", 
    x = "True Value of Mu", 
    y = "Power")
```

The plot shows a positive association between effect size and power. From the plot, we can see that as the value of mu increases the proportion of times the null hypothesis was rejected also increased. When mu reaches a value of 5 and 6, the power is 1. This could be interpreted as a larger effect size leading to higher power, meaning a higher rejection rate. 


## Plot Comparing the Average Estimate and the True Value of Mu

```{r, plot_2}
sim_results_2 |> 
  group_by(mu) |> 
  summarise(mean_estimate = mean(estimate)) |> 
  ggplot(aes(
    x = mu, 
    y = mean_estimate)) +
  geom_line(aes(
    color = "Average Estimate", 
    lty = "Average Estimate")) +
  geom_line(data = 
              sim_results_2 |>
              filter(p.value < 0.05) |> 
              group_by(mu) |> 
              summarise(mean_estimate = mean(estimate)),
            aes(color = "Rejected Average Estimate", lty = "Rejected Average Estimate")) +
  scale_color_manual(name = "", values = c("Average Estimate" = "blue", "Rejected Average Estimate" = "red")) +
  scale_linetype_manual(name = "", values = c("Average Estimate" = 2, "Rejected Average Estimate" = 1)) +
  labs(
    title = "Average Estimated Mean vs. True Mean",
    x = "True Mean (mu)",
    y = "Average Estimated Mean",
    color = "mu_hat") +
  theme_minimal()
```

The plot shows that the sample average of the mean (mu) in the tests where the null hypothesis was rejected is closely approximated to the true mean (mu). This is demonstrated with the overlay of lines for average estimate and conditional average estimate. This implies that when the null hypothesis is rejected, the t-test serves as an unbiased estimator of the population mean, aligning with the expected characteristics of the t-test within the specified simulation conditions. 


